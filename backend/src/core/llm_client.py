"""
Cohere client configuration for the RAG Chatbot Backend API
Handles connection to Cohere's language models and embedding services.
"""
import cohere
from typing import List, Dict, Any, Optional
from src.core.config import settings
from src.core.logging import get_logger
from src.core.exceptions import RAGException

logger = get_logger(__name__)


class LLMClient:
    """
    Client wrapper for Cohere API to handle both embeddings and text generation.
    """
    
    def __init__(self):
        if not settings.cohere_api_key:
            raise ValueError("Cohere API key is required but not provided in settings")
        
        # Initialize Cohere client
        self.client = cohere.Client(api_key=settings.cohere_api_key)
        self.embedding_model = "embed-english-v3.0"
        self.generation_model = "command-r-plus"  # High-performance model for generation
    
    def generate_text(self, 
                     prompt: str, 
                     context: Optional[str] = None, 
                     max_tokens: int = 500,
                     temperature: float = 0.3) -> str:
        """
        Generate text using Cohere's language model.
        
        Args:
            prompt: The input prompt for text generation
            context: Optional context to provide to the model
            max_tokens: Maximum number of tokens to generate
            temperature: Controls randomness (0.0-1.0)
            
        Returns:
            Generated text response
        """
        try:
            logger.debug(f"Generating text with prompt: {prompt[:100]}...")
            
            # Use the command-r-plus model for generation
            response = self.client.generate(
                model=self.generation_model,
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                stop_sequences=["Question:", "Context:"],
                return_likelihoods="NONE"
            )
            
            if response.generations and len(response.generations) > 0:
                generated_text = response.generations[0].text
                logger.info(f"Generated {len(generated_text)} characters of text")
                return generated_text.strip()
            else:
                logger.error("No text generated by Cohere API")
                raise RAGException("GENERATION_ERROR", "No text returned from generation API")
                
        except Exception as e:
            logger.error(f"Error generating text with Cohere: {str(e)}")
            raise RAGException("GENERATION_ERROR", f"Failed to generate text: {str(e)}")
    
    def generate_embedding(self, text: str) -> List[float]:
        """
        Generate an embedding for the given text using Cohere's embedding model.
        
        Args:
            text: The text to generate an embedding for
            
        Returns:
            A list of floats representing the embedding vector
        """
        try:
            logger.debug(f"Generating embedding for text of length {len(text)}")
            
            # Use the embed-english-v3.0 model for embeddings
            response = self.client.embed(
                texts=[text],
                model=self.embedding_model,
                input_type="search_document"  # Using search_document as it's optimized for document retrieval
            )
            
            if response.embeddings and len(response.embeddings) > 0:
                embedding = response.embeddings[0]
                logger.info(f"Generated embedding of dimension {len(embedding)}")
                return embedding
            else:
                logger.error("No embeddings returned from Cohere API")
                raise RAGException("EMBEDDING_ERROR", "No embeddings returned from API")
                
        except Exception as e:
            logger.error(f"Error generating embedding with Cohere: {str(e)}")
            raise RAGException("EMBEDDING_ERROR", f"Failed to generate embedding: {str(e)}")
    
    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for a batch of texts.
        
        Args:
            texts: A list of texts to generate embeddings for
            
        Returns:
            A list of embedding vectors (one for each input text)
        """
        try:
            logger.debug(f"Generating embeddings for {len(texts)} texts")
            
            # Cohere has limits on batch size, usually up to 96 texts per request
            batch_size = min(96, len(texts))
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                logger.debug(f"Processing embedding batch {i//batch_size + 1}/{(len(texts) - 1)//batch_size + 1}")
                
                response = self.client.embed(
                    texts=batch,
                    model=self.embedding_model,
                    input_type="search_document"
                )
                
                if response.embeddings:
                    all_embeddings.extend(response.embeddings)
                else:
                    logger.error(f"No embeddings returned for batch {i//batch_size + 1}")
                    # Add empty embeddings for the failed batch to maintain order
                    all_embeddings.extend([[] for _ in batch])
            
            logger.info(f"Generated embeddings for {len(texts)} texts in {len(range(0, len(texts), batch_size))} batch(es)")
            return all_embeddings
            
        except Exception as e:
            logger.error(f"Error generating embeddings batch: {str(e)}")
            raise RAGException("EMBEDDING_ERROR", f"Failed to generate embeddings batch: {str(e)}")


# Global instance for use throughout the application
llm_client = LLMClient()